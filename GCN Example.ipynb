{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "#We first define the message and reduce function as usual. Since the aggregation \n",
    "#on a node u only involves summing over the neighbors’ representations hv, \n",
    "#we can simply use builtin functions:\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We then proceed to define the GCNLayer module. \n",
    "#A GCNLayer essentially performs message passing on all \n",
    "#the nodes then applies a fully-connected layer.\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Creating a local scope so that all the stored ndata and edata\n",
    "        # (such as the `'h'` ndata below) are automatically popped out\n",
    "        # when the scope exits.\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = feature\n",
    "            g.update_all(gcn_msg, gcn_reduce)\n",
    "            h = g.ndata['h']\n",
    "            return self.linear(h)\n",
    "\n",
    "#The forward function is essentially the same as any other commonly seen NNs model in PyTorch. \n",
    "#We can initialize GCN like any nn.Module. For example, let’s define a simple neural network consisting \n",
    "#of two GCN layers. Suppose we are training the classifier for the cora dataset (the input feature size \n",
    "#is 1433 and the number of classes is 7). \n",
    "#The last GCN layer computes node embeddings, so the last layer in general does not apply activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): GCNLayer(\n",
      "    (linear): Linear(in_features=1433, out_features=16, bias=True)\n",
      "  )\n",
      "  (layer2): GCNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = GCNLayer(1433, 16)\n",
    "        self.layer2 = GCNLayer(16, 7)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.layer1(g, features))\n",
    "        x = self.layer2(g, x)\n",
    "        return x\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load the cora dataset using DGL’s built-in data module\n",
    "\n",
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    train_mask = th.BoolTensor(data.train_mask)\n",
    "    test_mask = th.BoolTensor(data.test_mask)\n",
    "    g = DGLGraph(data.graph)\n",
    "    return g, features, labels, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When a model is trained, we can use the following method to evaluate \n",
    "#zGCthe performance of the model on the test dataset:\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache failed, re-processing.\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "Epoch 00000 | Loss 1.9650 | Test Acc 0.1750 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.8394 | Test Acc 0.2220 | Time(s) nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rothg\\Anaconda3\\lib\\site-packages\\dgl\\data\\utils.py:285: UserWarning: Property dataset.feat will be deprecated, please use g.ndata['feat'] instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "C:\\Users\\rothg\\Anaconda3\\lib\\site-packages\\dgl\\data\\utils.py:285: UserWarning: Property dataset.label will be deprecated, please use g.ndata['label'] instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "C:\\Users\\rothg\\Anaconda3\\lib\\site-packages\\dgl\\data\\utils.py:285: UserWarning: Property dataset.train_mask will be deprecated, please use g.ndata['train_mask'] instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "C:\\Users\\rothg\\Anaconda3\\lib\\site-packages\\dgl\\data\\utils.py:285: UserWarning: Property dataset.test_mask will be deprecated, please use g.ndata['test_mask'] instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "C:\\Users\\rothg\\Anaconda3\\lib\\site-packages\\dgl\\data\\utils.py:285: UserWarning: Property dataset.graph will be deprecated, please use dataset.g instead.\n",
      "  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))\n",
      "C:\\Users\\rothg\\Anaconda3\\lib\\site-packages\\dgl\\base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n",
      "C:\\Users\\rothg\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\rothg\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00002 | Loss 1.7255 | Test Acc 0.3640 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.6197 | Test Acc 0.4270 | Time(s) 0.0209\n",
      "Epoch 00004 | Loss 1.5261 | Test Acc 0.4950 | Time(s) 0.0209\n",
      "Epoch 00005 | Loss 1.4465 | Test Acc 0.5540 | Time(s) 0.0199\n",
      "Epoch 00006 | Loss 1.3763 | Test Acc 0.5910 | Time(s) 0.0194\n",
      "Epoch 00007 | Loss 1.3092 | Test Acc 0.6160 | Time(s) 0.0191\n",
      "Epoch 00008 | Loss 1.2420 | Test Acc 0.6460 | Time(s) 0.0193\n",
      "Epoch 00009 | Loss 1.1749 | Test Acc 0.6640 | Time(s) 0.0191\n",
      "Epoch 00010 | Loss 1.1082 | Test Acc 0.6830 | Time(s) 0.0188\n",
      "Epoch 00011 | Loss 1.0424 | Test Acc 0.7030 | Time(s) 0.0186\n",
      "Epoch 00012 | Loss 0.9795 | Test Acc 0.7120 | Time(s) 0.0186\n",
      "Epoch 00013 | Loss 0.9205 | Test Acc 0.7050 | Time(s) 0.0185\n",
      "Epoch 00014 | Loss 0.8649 | Test Acc 0.7070 | Time(s) 0.0187\n",
      "Epoch 00015 | Loss 0.8117 | Test Acc 0.7100 | Time(s) 0.0186\n",
      "Epoch 00016 | Loss 0.7602 | Test Acc 0.7160 | Time(s) 0.0185\n",
      "Epoch 00017 | Loss 0.7112 | Test Acc 0.7250 | Time(s) 0.0184\n",
      "Epoch 00018 | Loss 0.6653 | Test Acc 0.7300 | Time(s) 0.0184\n",
      "Epoch 00019 | Loss 0.6230 | Test Acc 0.7270 | Time(s) 0.0183\n",
      "Epoch 00020 | Loss 0.5840 | Test Acc 0.7250 | Time(s) 0.0185\n",
      "Epoch 00021 | Loss 0.5476 | Test Acc 0.7250 | Time(s) 0.0184\n",
      "Epoch 00022 | Loss 0.5136 | Test Acc 0.7210 | Time(s) 0.0183\n",
      "Epoch 00023 | Loss 0.4822 | Test Acc 0.7210 | Time(s) 0.0183\n",
      "Epoch 00024 | Loss 0.4533 | Test Acc 0.7230 | Time(s) 0.0183\n",
      "Epoch 00025 | Loss 0.4266 | Test Acc 0.7210 | Time(s) 0.0183\n",
      "Epoch 00026 | Loss 0.4019 | Test Acc 0.7190 | Time(s) 0.0184\n",
      "Epoch 00027 | Loss 0.3789 | Test Acc 0.7180 | Time(s) 0.0184\n",
      "Epoch 00028 | Loss 0.3573 | Test Acc 0.7150 | Time(s) 0.0184\n",
      "Epoch 00029 | Loss 0.3370 | Test Acc 0.7180 | Time(s) 0.0184\n",
      "Epoch 00030 | Loss 0.3177 | Test Acc 0.7200 | Time(s) 0.0184\n",
      "Epoch 00031 | Loss 0.2995 | Test Acc 0.7220 | Time(s) 0.0183\n",
      "Epoch 00032 | Loss 0.2823 | Test Acc 0.7240 | Time(s) 0.0184\n",
      "Epoch 00033 | Loss 0.2661 | Test Acc 0.7230 | Time(s) 0.0184\n",
      "Epoch 00034 | Loss 0.2509 | Test Acc 0.7220 | Time(s) 0.0184\n",
      "Epoch 00035 | Loss 0.2367 | Test Acc 0.7250 | Time(s) 0.0183\n",
      "Epoch 00036 | Loss 0.2233 | Test Acc 0.7270 | Time(s) 0.0183\n",
      "Epoch 00037 | Loss 0.2108 | Test Acc 0.7260 | Time(s) 0.0183\n",
      "Epoch 00038 | Loss 0.1991 | Test Acc 0.7260 | Time(s) 0.0184\n",
      "Epoch 00039 | Loss 0.1882 | Test Acc 0.7260 | Time(s) 0.0184\n",
      "Epoch 00040 | Loss 0.1779 | Test Acc 0.7240 | Time(s) 0.0184\n",
      "Epoch 00041 | Loss 0.1683 | Test Acc 0.7260 | Time(s) 0.0184\n",
      "Epoch 00042 | Loss 0.1593 | Test Acc 0.7270 | Time(s) 0.0184\n",
      "Epoch 00043 | Loss 0.1509 | Test Acc 0.7290 | Time(s) 0.0184\n",
      "Epoch 00044 | Loss 0.1430 | Test Acc 0.7300 | Time(s) 0.0185\n",
      "Epoch 00045 | Loss 0.1357 | Test Acc 0.7280 | Time(s) 0.0185\n",
      "Epoch 00046 | Loss 0.1289 | Test Acc 0.7280 | Time(s) 0.0184\n",
      "Epoch 00047 | Loss 0.1225 | Test Acc 0.7300 | Time(s) 0.0184\n",
      "Epoch 00048 | Loss 0.1165 | Test Acc 0.7290 | Time(s) 0.0185\n",
      "Epoch 00049 | Loss 0.1109 | Test Acc 0.7270 | Time(s) 0.0185\n"
     ]
    }
   ],
   "source": [
    "#We then train the model as follows:\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "g, features, labels, train_mask, test_mask = load_cora_data()\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-2)\n",
    "dur = []\n",
    "for epoch in range(50):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    net.train()\n",
    "    logits = net(g, features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(net, g, features, labels, test_mask)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
